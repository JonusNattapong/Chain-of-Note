{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Note RAG Tutorial\n",
    "\n",
    "This notebook provides a step-by-step tutorial on how to use the Chain-of-Note RAG system to reduce hallucinations in AI-generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "from src.data_loader import DocumentLoader\n",
    "from src.embeddings import SentenceTransformerEmbeddings\n",
    "from src.document_store import DocumentStore\n",
    "from src.chain_of_note import ChainOfNote\n",
    "from src.rag_system import ChainOfNoteRAG\n",
    "from src.evaluation import RAGEvaluator\n",
    "\n",
    "print(\"Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Documents\n",
    "\n",
    "Let's create some sample documents for our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"content\": \"Chain-of-Note is a technique for improving RAG systems by generating intermediate notes. By first creating structured notes from retrieved documents, the model can better organize information before generating a response. This reduces hallucinations and improves factual accuracy.\",\n",
    "        \"metadata\": {\"source\": \"Research Paper\", \"topic\": \"Chain-of-Note\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Hallucinations in large language models occur when the model generates information that seems plausible but is factually incorrect or not grounded in the provided context. This is a common issue in generative AI that researchers are actively working to address.\",\n",
    "        \"metadata\": {\"source\": \"AI Blog\", \"topic\": \"Hallucinations\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Retrieval-Augmented Generation (RAG) combines retrieval mechanisms with text generation to produce outputs that are grounded in external knowledge. By retrieving relevant information before generation, RAG can improve factuality compared to standard language models.\",\n",
    "        \"metadata\": {\"source\": \"Academic Paper\", \"topic\": \"RAG\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the RAG System\n",
    "\n",
    "Now, let's initialize our Chain-of-Note RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "print(\"Initializing embedding model...\")\n",
    "embedding_model = SentenceTransformerEmbeddings()\n",
    "\n",
    "# Initialize the RAG system\n",
    "print(\"Initializing Chain-of-Note RAG system...\")\n",
    "rag_system = ChainOfNoteRAG(embedding_model=embedding_model)\n",
    "\n",
    "print(\"System initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Documents to the RAG System\n",
    "\n",
    "Let's add our sample documents to the RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the RAG system\n",
    "rag_system.add_documents(documents)\n",
    "print(f\"Added {len(documents)} documents to the RAG system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query the RAG System\n",
    "\n",
    "Now let's query our system with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query\n",
    "query = \"How does Chain-of-Note help reduce hallucinations in RAG systems?\"\n",
    "\n",
    "# Query the system\n",
    "response = rag_system.query(\n",
    "    query=query,\n",
    "    top_k=3,\n",
    "    return_context=True,\n",
    "    return_notes=True\n",
    ")\n",
    "\n",
    "# Display the notes\n",
    "print(\"Generated Notes:\")\n",
    "print(\"-\" * 60)\n",
    "print(response[\"notes\"])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display the answer\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(\"-\" * 60)\n",
    "print(response[\"answer\"])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Retrieved Documents\n",
    "\n",
    "Let's see which documents were retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"], 1):\n",
    "    print(f\"Document {i} (Score: {doc['score']:.4f}):\")\n",
    "    print(f\"Topic: {doc['metadata'].get('topic', 'Unknown')}\")\n",
    "    print(f\"Source: {doc['metadata'].get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {doc['content']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Standard RAG\n",
    "\n",
    "Now let's implement a basic RAG approach without the Chain-of-Note technique for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic RAG implementation without notes\n",
    "def basic_rag(query, documents, model):\n",
    "    prompt = f\"Question: {query}\\n\\nContext Information:\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        content = doc[\"content\"]\n",
    "        prompt += f\"[Document {i}]: {content}\\n\\n\"\n",
    "        \n",
    "    prompt += \"Based on the above context information, please answer the question concisely.\\n\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer directly\n",
    "    result = model.generator(prompt, max_length=512, num_return_sequences=1)\n",
    "    answer = result[0][\"generated_text\"]\n",
    "    \n",
    "    return answer.strip()\n",
    "\n",
    "# Get standard RAG answer\n",
    "standard_answer = basic_rag(query, response[\"context\"], rag_system.chain_of_note)\n",
    "\n",
    "print(\"Standard RAG Answer:\")\n",
    "print(\"-\" * 60)\n",
    "print(standard_answer)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nChain-of-Note RAG Answer:\")\n",
    "print(\"-\" * 60)\n",
    "print(response[\"answer\"])\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Responses\n",
    "\n",
    "Let's evaluate both approaches to see which one performs better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "# Prepare reference docs for evaluation\n",
    "reference_docs = [doc[\"content\"] for doc in response[\"context\"]]\n",
    "\n",
    "# Compare systems\n",
    "comparison = evaluator.compare_systems(\n",
    "    query=query,\n",
    "    standard_rag_response=standard_answer,\n",
    "    chain_of_note_response=response[\"answer\"],\n",
    "    reference_docs=reference_docs\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a comparison table\n",
    "metrics = {\n",
    "    \"Hallucination Score\": [\n",
    "        comparison[\"standard_rag\"][\"hallucination_score\"],\n",
    "        comparison[\"chain_of_note_rag\"][\"hallucination_score\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "if \"relevance\" in comparison[\"standard_rag\"]:\n",
    "    metrics[\"Relevance\"] = [\n",
    "        comparison[\"standard_rag\"][\"relevance\"],\n",
    "        comparison[\"chain_of_note_rag\"][\"relevance\"]\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(metrics, index=[\"Standard RAG\", \"Chain-of-Note RAG\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "ax = df.plot(kind=\"bar\", figsize=(10, 6))\n",
    "plt.title(\"Comparison: Standard RAG vs Chain-of-Note RAG\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Add value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "As shown in the evaluation, Chain-of-Note RAG typically performs better than standard RAG in terms of hallucination reduction. The intermediate note-taking step helps the model organize information from the retrieved documents before generating a response, leading to more accurate and factual answers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
